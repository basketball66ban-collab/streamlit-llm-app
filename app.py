from dotenv import load_dotenv
load_dotenv()

import os
import streamlit as st
from langchain_community.chat_models import ChatOpenAI  # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¯¾å¿œ
from langchain.schema import SystemMessage, HumanMessage

# --- LLM ã®åˆæœŸåŒ– ---
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

# --- å°‚é–€å®¶å›ç­”é–¢æ•° ---
def get_expert_response(user_input: str, expert: str) -> str:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã¨å°‚é–€å®¶ã®é¸æŠã«å¿œã˜ã¦LLMã®å›ç­”ã‚’è¿”ã™"""
    
    if expert == "å°‚é–€å®¶Aï¼ˆæ „é¤Šå£«ï¼‰":
        system_prompt = (
            "ã‚ãªãŸã¯å„ªç§€ãªæ „é¤Šå£«ã§ã™ã€‚"
            "é£Ÿäº‹ã€æ „é¤Šãƒãƒ©ãƒ³ã‚¹ã€å¥åº·ç®¡ç†ã«ã¤ã„ã¦å°‚é–€çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        )
    else:  # å°‚é–€å®¶Bï¼ˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼‰
        system_prompt = (
            "ã‚ãªãŸã¯å„ªç§€ãªãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã§ã™ã€‚"
            "é‹å‹•ã€ç­‹ãƒˆãƒ¬ã€å§¿å‹¢æ”¹å–„ã«ã¤ã„ã¦å°‚é–€çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        )

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ]

    result = llm(messages)
    return result.content

# --- Streamlit UI ---
st.title("ğŸ’¬ 2äººã®å°‚é–€å®¶ã«ç›¸è«‡ã‚¢ãƒ—ãƒª")

st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€2äººã®å°‚é–€å®¶ã«è³ªå•ã‚’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚  
1. ä¸Šã®ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„  
2. å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«è³ªå•ã‚’å…¥åŠ›ã—ã€ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„  
3. é¸ã‚“ã å°‚é–€å®¶ã¨ã—ã¦ã®å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
""")

# å°‚é–€å®¶ã®é¸æŠ
expert_choice = st.radio(
    "ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„ï¼š",
    ["å°‚é–€å®¶Aï¼ˆæ „é¤Šå£«ï¼‰", "å°‚é–€å®¶Bï¼ˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼‰"]
)

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
with st.form("user_input_form"):
    user_input = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    submitted = st.form_submit_button("é€ä¿¡")

    if submitted and user_input.strip():
        response = get_expert_response(user_input, expert_choice)
        st.markdown("### ğŸ§‘â€âš•ï¸ å›ç­”")
        st.write(response)
